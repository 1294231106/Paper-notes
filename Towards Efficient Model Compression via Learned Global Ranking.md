#  Towards Efficient Model Compression via Learned Global Ranking  #
———————————————————————————————
##  摘要 ##
- 神经网络剪枝为深度神经网络在资源受限设备上的应用提供了广阔的前景。然而，现有的剪枝方法由于缺乏对非显著网络成分的理论指导，在剪枝设计中存在训练效率低、人工成本高的问题。
- 本文通过对高秩特征图的研究，提出了一种新的滤波剪枝方法。我们的HRank的灵感来自于这样一个发现，即由单个过滤器生成的多个特征图的平均秩总是相同的，而不考虑接收到的CNNs图像batch的数量。在此基础上，本文提出了一种用数学方法对低秩特征图进行滤波的方法。我们的剪枝背后的原则是，低秩特征图包含的信息较少，因此剪枝的结果可以很容易地复制。
- 此外，我们还通过实验证明了高秩特征图的权值包含了更多的重要信息，即使不更新一部分，对模型性能的影响也很小。在不引入任何额外约束的情况下，HRank在计算和参数减少方面比现有技术有了显著的改进，并且具有相似的准确性。
##  Introduction ##
### 先前的方法 ###
- 1.卷积核压缩
- 2.参数量化
- 3.网络剪枝，包含权值剪枝和滤波器剪枝

我们的方法主要是滤波器剪枝

### 滤波器剪枝的分类 ###
- 属性重要性:根据CNNs的固有属性对过滤器进行修剪。这些修剪方法并不能弥补网络训练的损失。在修剪之后，通过微调来增强模型性能。主要有：利用大网络中输出的稀疏性来去除零激活率高的滤波器。基于l1-norm的剪枝假设带有小规范的参数或特征信息较少并删除了最不重要的滤波器。He等人计算了层的几何中位数，并修剪了最接近的过滤器
- 适应重要性:与基于属性重要性的方法不同，另一个方向将剪枝需求嵌入到网络训练损失中，并采用联合再训练优化来生成自适应剪枝决策。Liu等人和Zhao等人对BN层的尺度因子施加了稀疏约束，使得具有较低尺度因子的通道被认为是不重要的。Huang等人和Lin等人引入了一个新的比例因子参数(也称为掩码)来学习稀疏结构剪枝，其中移除对应于比例因子0的滤波器。


### 我们的方法 ###
- 如图2所示，我们发现，无论CNN看到多少数据，单个过滤器生成的feature map的平均秩总是相同的。这表明，只使用输入图像的一小部分就可以准确地估计深度cnn中特征图的秩，从而达到高效的目的。基于这一思想，我们用数学方法证明了秩越低的特征映射对精度的贡献越小。因此，可以首先删除生成这些特征映射的过滤器。

summarize
- 我们的经验证明，单个过滤器生成的特征图的平均秩几乎是不变的。
据我们所知，这是第一次有这样的结论
- 我们从数学上证明了具有较低等级特征图的过滤器信息较少,因此可以将其删除掉
- 实验的结果非常好